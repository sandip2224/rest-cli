{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_hands_on_Sakhyaa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-dwIkkIXDe"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojfIGzHtM251"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOEgVpFNP3v",
        "outputId": "9d6c6bc7-2680-422b-c63e-dc24d39a96b0"
      },
      "source": [
        "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\"\n",
        "text1= NER(raw_text)\n",
        "\n",
        "for word in text1.ents:\n",
        "    print(word.text,word.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Indian Space Research Organisation ORG\n",
            "the national space agency ORG\n",
            "India GPE\n",
            "Bengaluru GPE\n",
            "Department of Space ORG\n",
            "India GPE\n",
            "ISRO ORG\n",
            "DOS ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HueDO2hSIh5G"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT3f3MfOIvnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe87138b-11f0-4f54-da57-50290361ac56"
      },
      "source": [
        "   word=\"computation\"\n",
        "   stemword=ps.stem(word)\n",
        "   print(stemword)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cg-X1i2JM5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e5c9e7-62c5-44c5-ee4c-568134610a23"
      },
      "source": [
        "# import these modules \n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfSqKXfqKovt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "9fb4e15b-38c5-4ca1-dcbb-ec0023581821"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer() \n",
        "print(\"crying : \", lemmatizer.lemmatize(\"computation\")) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bffac59d6b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"crying : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"computation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlgdo5PkK0ZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9a41f0-2f6e-4e56-d988-a9d3a1818ed5"
      },
      "source": [
        " nltk.download('averaged_perceptron_tagger')  \n",
        " nltk.download('punkt')\n",
        " from nltk.tokenize import word_tokenize\n",
        " text = word_tokenize(\"Butterfly is beautiful \")\n",
        " print(text)\n",
        " nltk.pos_tag(text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Butterfly', 'is', 'beautiful']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Butterfly', 'NNP'), ('is', 'VBZ'), ('beautiful', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT8aCWlIlBOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf7e472-ad74-4973-f528-f3fa9fac697f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "word= wordnet.synsets(\"good\")\n",
        "print(\"The Word it self: \",word[2].name())\n",
        "print(\"Examples: \",word[2].examples())\n",
        "print(\"Example Usage: \",word[2].definition())      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "The Word it self:  good.n.03\n",
            "Examples:  ['weigh the good against the bad', 'among the highest goods of all are happiness and self-realization']\n",
            "Example Usage:  that which is pleasing or valuable or useful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXnnHRWzxcLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e3f727-fa76-4220-8743-622f26596d7d"
      },
      "source": [
        "synonyms = []\n",
        "antonyms = []\n",
        "for syn in wordnet.synsets(\"good\"): \n",
        "  for l in syn.lemmas():\n",
        "    synonyms.append(l.name())\n",
        "    if l.antonyms():\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "print(\"The Meanings ....:\",set(synonyms))\n",
        "print(\"The Opposites ....\",set(antonyms))            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Meanings ....: {'skilful', 'beneficial', 'secure', 'respectable', 'honest', 'unspoilt', 'unspoiled', 'safe', 'near', 'expert', 'ripe', 'sound', 'thoroughly', 'estimable', 'skillful', 'serious', 'right', 'effective', 'goodness', 'well', 'dear', 'honorable', 'trade_good', 'full', 'dependable', 'undecomposed', 'in_force', 'adept', 'soundly', 'just', 'in_effect', 'practiced', 'commodity', 'salutary', 'upright', 'good', 'proficient'}\n",
            "The Opposites .... {'ill', 'bad', 'evilness', 'badness', 'evil'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px_aImfcR3Vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d516194-96bc-45b5-eb44-8d601e11cf0c"
      },
      "source": [
        " nltk.download('averaged_perceptron_tagger')  \n",
        " nltk.download('punkt')\n",
        " nltk.download('maxent_ne_chunker')\n",
        " nltk.download('words')\n",
        " from nltk.tokenize import word_tokenize\n",
        " text = word_tokenize(\"Jim bought 300 shares of Acme Corp. in 2006\")\n",
        " posttagwors=nltk.pos_tag(text)\n",
        " namedentity=nltk.ne_chunk(posttagwors,binary=False)\n",
        " print(namedentity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Jim/NNP)\n",
            "  bought/VBD\n",
            "  300/CD\n",
            "  shares/NNS\n",
            "  of/IN\n",
            "  (ORGANIZATION Acme/NNP Corp./NNP)\n",
            "  in/IN\n",
            "  2006/CD)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCd5HlgTXZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d4b8d5-379a-4984-a870-efcee23982ee"
      },
      "source": [
        "!pip install gTTS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.2.3-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gTTS) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from gTTS) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gTTS) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2.10)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4aFBYV9Tbc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcdaf6b-84d3-4886-b912-4f33bc6fca4b"
      },
      "source": [
        "import gtts\n",
        "!pip install playsound\n",
        "from playsound import playsound"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playsound\n",
            "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
            "Building wheels for collected packages: playsound\n",
            "  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7037 sha256=0f57eb47a25f76b758d2106bdbc05a5fbd8a3527f51cd93142c78dda81e2addd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/f8/bb/ea57c0146b664dca3a0ada4199b0ecb5f9dfcb7b7e22b65ba2\n",
            "Successfully built playsound\n",
            "Installing collected packages: playsound\n",
            "Successfully installed playsound-1.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXkvXVYiBdQn"
      },
      "source": [
        "tts1 = gtts.gTTS(\"I like NLP\", lang=\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpmFbypdDf2F"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEKkPEttCbXR"
      },
      "source": [
        "tts1.save(\"sample.mp3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uokD4fp1BNb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_WBbmZM_tmQ"
      },
      "source": [
        "tts2=gtts.gTTS(text=\"यह हिंदी में एक उदाहरण है\",lang='hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lryrEW0l_vty"
      },
      "source": [
        "tts2.save(\"sample2.mp3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DIpkBfMTljO"
      },
      "source": [
        "tts3 = gtts.gTTS(\"இன்று நல்ல நாள் \", lang=\"ta\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2G5WdQYToo2"
      },
      "source": [
        "tts3.save(\"sample.mp3\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}